{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import requests\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set url\n",
    "url_news = 'https://mars.nasa.gov/news/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set exe path and browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.visit(url_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML object\n",
    "html_news = browser.html\n",
    "# Create BeautifulSoup object; parse with lxml\n",
    "soupy = bs(html_news, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Hear Audio From NASA's Perseverance As It Travels Through Deep Space:\n",
      "-------------------\n",
      "The first to be rigged with microphones, the agency's latest Mars rover picked up the subtle sounds of its own inner workings during interplanetary flight.\n",
      "-------------------\n",
      "Published: November 18, 2020\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the latest news title and paragraph\n",
    "title = soupy.find('div', class_=\"bottom_gradient\").h3.text\n",
    "para = soupy.find('div', class_=\"article_teaser_body\").text\n",
    "date = soupy.find('div', class_=\"list_date\").text\n",
    "print('-------------------')\n",
    "print(f'{title}:') \n",
    "print('-------------------')\n",
    "print(para) \n",
    "print('-------------------')\n",
    "print(f'Published: {date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set search url\n",
    "url_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base feature image url\n",
    "base_img_url ='https://www.jpl.nasa.gov/spaceimages/images/largesize/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set exe path and browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.visit(url_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML object\n",
    "html_img = browser.html\n",
    "# Create BeautifulSoup object; parse with lxml\n",
    "soupimg = bs(html_img, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars/spaceimages/images/mediumsize/PIA18906_ip.jpg\n"
     ]
    }
   ],
   "source": [
    "# retrieve current Featured Mars Image\n",
    "image_find = soupimg.find('a', class_='button fancybox')['data-fancybox-href']\n",
    "image_pg = (f'{url_img}{image_find}')\n",
    "print(image_pg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visit next page for feature image name and blurb\n",
    "browser.visit(image_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Shines in High-Energy X-rays\n",
      "X-rays stream off the sun in this first picture of the sun, overlaid on a picture taken by NASA's Solar Dynamics Observatory, taken by NuSTAR.\n"
     ]
    }
   ],
   "source": [
    "#get name and blurb about feature image\n",
    "image_name = soupimg.find('a', class_='button fancybox')['data-title']\n",
    "image_desc = soupimg.find('a', class_='button fancybox')['data-description']\n",
    "\n",
    "print(image_name)\n",
    "print(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA18906_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# get feature id by it's string position and make feature img url\n",
    "jpg = 'hires.jpg'\n",
    "end=len(image_find)-6\n",
    "start1= image_find.find(\"z\")\n",
    "start=start1+3\n",
    "img_id=image_find[start:end]\n",
    "feature_image_url = f'{base_img_url}{img_id}{jpg}'\n",
    "print(feature_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url\n",
    "url_facts = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Panda's `read_html` to parse the url\n",
    "table = pd.read_html(url_facts)\n",
    "#table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of facts\n",
    "mars_facts_df = table[2]\n",
    "mars_facts_df.columns = [\"What\", \"Fact\"]\n",
    "mars_facts_df.set_index(\"What\", inplace=True)\n",
    "mars_facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to html table string\n",
    "mars_html_table1 = mars_facts_df.to_html()\n",
    "#mars_html_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove carrige return \\n\n",
    "mars_html_table = mars_html_table1.replace('\\n', '')\n",
    "print(mars_html_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set url\n",
    "url_hem = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set base url to append hemi image download tif file name to\n",
    "base_hemi_url='https://astropedia.astrogeology.usgs.gov/download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set exe path and browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.visit(url_hem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML object\n",
    "html_hem = browser.html\n",
    "# Create BeautifulSoup object; parse with lxml\n",
    "souphem = bs(html_hem, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Cerberus Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif'}]\n"
     ]
    }
   ],
   "source": [
    "# loop to gather all full res hemisphere img urls\n",
    "start_url = 'https://astrogeology.usgs.gov'\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Retrieve the parent divs for all articles\n",
    "results_all = souphem.find('div', class_='collapsible results')\n",
    "hemispheres = results_all.find_all('div', class_='item')\n",
    "\n",
    "# loop over results to get hemi title and img url\n",
    "for hemi in hemispheres:\n",
    "    # scrape the link hemi title\n",
    "    title_hemi = hemi.find('div', class_='description').h3.text\n",
    "    # scrape the link to hemi img\n",
    "    img_url = hemi.find('div', class_='description').a[\"href\"]\n",
    "    #get end of url for tif download file\n",
    "    start_hemi = img_url.find(\"p/\")+1\n",
    "    end_hemi = len(img_url)\n",
    "    for_tif=img_url[start_hemi:end_hemi]\n",
    "    img_hemi_url=f'{base_hemi_url}{for_tif}.tif'\n",
    "    \n",
    "    \n",
    "    # make title and img_url into dict and store in list\n",
    "    hemi_dict = {}\n",
    "    hemi_dict['title'] = title_hemi\n",
    "    hemi_dict['img_url'] = img_hemi_url\n",
    "    \n",
    "    hemisphere_image_urls.append(hemi_dict)\n",
    "\n",
    "print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a dictionary of all the scraped mars data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that holds all the scraped data\n",
    "mars_scrpd = {}\n",
    "mars_scrpd[\"news_title\"] = title\n",
    "mars_scrpd[\"news_paragraph\"] = para\n",
    "mars_scrpd[\"news_date\"] = date\n",
    "mars_scrpd[\"feature_image_url\"] = feature_image_url\n",
    "mars_scrpd[\"feature_image_name\"] = image_name\n",
    "mars_scrpd[\"feature_image_described\"] = image_desc\n",
    "mars_scrpd[\"mars_facts\"] = mars_html_table\n",
    "mars_scrpd[\"hemisphere_image_urls\"] = hemisphere_image_urls\n",
    "mars_scrpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
